# Actual Workflow Architecture - SentinelAI Audit Framework

## ğŸ¯ **Current Reality vs. Proposed Architecture**

### **A/ PDF Processing Requirements**

#### **Multiple Document Types with Different Processors**
```
shared_data/documents/
â”œâ”€â”€ PCI-DSS-v4_0_1.pdf              # âœ… Compliance Standards (306 controls)
â”œâ”€â”€ AWSOperationalBestPracticesPCIDSS4.pdf  # ğŸš§ Implementation Guidelines  
â””â”€â”€ [future compliance PDFs]        # ğŸ”® ISO27001, NIST CSF, etc.
```

#### **Current vs. Needed Processors**
```python
# âœ… CURRENT: PCI Standards Processor
data_pipeline/processors/compliance/pci_dss/
â”œâ”€â”€ control_extractor.py           # Extracts 306 controls
â”œâ”€â”€ table_parser.py               # Handles complex tables
â””â”€â”€ quality_validator.py          # 95% quality score

# ğŸš§ NEEDED: AWS Best Practices Processor  
data_pipeline/processors/implementation/aws_practices/
â”œâ”€â”€ practice_extractor.py         # Extract implementation patterns
â”œâ”€â”€ code_snippet_parser.py        # Parse CloudFormation/code examples
â””â”€â”€ mapping_generator.py          # Map practices to PCI controls

# ğŸ”® FUTURE: Additional Processors
data_pipeline/processors/compliance/iso27001/
data_pipeline/processors/implementation/azure_practices/
```

### **B/ Evidence Collection Requirements**

#### **Multi-Account Config Collection**
```python
# ğŸš§ TO BE BUILT: Enhanced Evidence Collector
services/evidence_collector/
â”œâ”€â”€ collectors/
â”‚   â”œâ”€â”€ aws_config_collector.py   # Collect from multiple AWS accounts
â”‚   â”œâ”€â”€ azure_collector.py        # Future: Azure configs  
â”‚   â””â”€â”€ gcp_collector.py          # Future: GCP configs
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ config_normalizer.py      # Standardize different formats
â”‚   â”œâ”€â”€ compliance_mapper.py      # Map configs to requirements
â”‚   â””â”€â”€ evidence_validator.py     # Validate collected evidence
â””â”€â”€ storage/
    â”œâ”€â”€ s3_uploader.py            # Archive to S3
    â””â”€â”€ db_ingestion.py           # Store in PostgreSQL
```

## ğŸ—ï¸ **Optimized Architecture for Your Workflow**

### **Phase 1: Document Processing Pipeline**

```
data_pipeline/
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ documents/               # Document-type-specific processors
â”‚   â”‚   â”œâ”€â”€ compliance_standards/    # PCI DSS, ISO27001, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ pci_dss_processor.py     # âœ… Current (306 controls)
â”‚   â”‚   â”‚   â””â”€â”€ iso27001_processor.py    # ğŸ”® Future
â”‚   â”‚   â”œâ”€â”€ implementation_guides/      # AWS, Azure best practices
â”‚   â”‚   â”‚   â”œâ”€â”€ aws_practices_processor.py   # ğŸš§ Needed
â”‚   â”‚   â”‚   â””â”€â”€ azure_practices_processor.py # ğŸ”® Future
â”‚   â”‚   â””â”€â”€ shared/                     # Common utilities
â”‚   â”‚       â”œâ”€â”€ pdf_extractor.py       # PDF text extraction
â”‚   â”‚       â”œâ”€â”€ table_parser.py        # Table structure detection
â”‚   â”‚       â””â”€â”€ content_chunker.py     # Semantic chunking
â”‚   â””â”€â”€ evidence/                # Evidence collection processing
â”‚       â”œâ”€â”€ config_processors/       # Config-specific processing
â”‚       â”‚   â”œâ”€â”€ aws_config_processor.py
â”‚       â”‚   â””â”€â”€ compliance_mapper.py
â”‚       â””â”€â”€ validators/             # Evidence validation
â”œâ”€â”€ formatters/
â”‚   â”œâ”€â”€ bedrock/                # Knowledge Base formatting
â”‚   â”‚   â”œâ”€â”€ semantic_chunker.py     # Optimize for embeddings
â”‚   â”‚   â””â”€â”€ metadata_generator.py   # KB metadata
â”‚   â”œâ”€â”€ database/               # PostgreSQL formatting  
â”‚   â”‚   â”œâ”€â”€ schema_generator.py     # Create DB schemas
â”‚   â”‚   â”œâ”€â”€ csv_formatter.py       # Database-ready CSV
â”‚   â”‚   â””â”€â”€ json_formatter.py      # Structured JSON
â”‚   â””â”€â”€ storage/                # S3 archive formatting
â”‚       â”œâ”€â”€ evidence_packager.py   # Package evidence sets
â”‚       â””â”€â”€ metadata_indexer.py    # Create searchable indexes
â””â”€â”€ destinations/
    â”œâ”€â”€ bedrock/                # Bedrock Knowledge Base
    â”œâ”€â”€ database/               # Aurora PostgreSQL
    â””â”€â”€ storage/                # S3 Evidence Store
```

### **Phase 2: Evidence Collection Integration**

```python
# services/evidence_collector/main.py
class EvidenceCollector:
    def collect_aws_configs(self, accounts: List[str]) -> EvidenceSet:
        """Collect configs from multiple AWS accounts."""
        evidence = EvidenceSet()
        
        for account in accounts:
            # Use boto3 to collect various AWS configs
            ec2_configs = self.collect_ec2_configs(account)
            vpc_configs = self.collect_vpc_configs(account) 
            iam_configs = self.collect_iam_configs(account)
            
            evidence.add_account_data(account, {
                'ec2': ec2_configs,
                'vpc': vpc_configs,
                'iam': iam_configs
            })
        
        return evidence
    
    def process_and_store(self, evidence: EvidenceSet):
        """Process evidence and store in dual format."""
        # 1. Normalize and validate
        processor = ConfigProcessor()
        normalized = processor.normalize_evidence(evidence)
        
        # 2. Format for PostgreSQL
        db_formatter = DatabaseFormatter()
        db_data = db_formatter.format_for_postgres(normalized)
        
        # 3. Format for S3 storage
        s3_formatter = StorageFormatter()
        s3_package = s3_formatter.package_evidence(normalized)
        
        # 4. Store in both destinations
        DatabaseIngestion().store_evidence(db_data)
        S3Storage().upload_evidence_package(s3_package)
```

## ğŸ”„ **Workflow Orchestration**

### **A/ PDF Processing Workflow**
```python
# orchestration/workflows/document_workflow.py
class DocumentProcessingWorkflow:
    
    def process_compliance_standard(self, pdf_path: str) -> ProcessingResult:
        """Process compliance standards like PCI DSS."""
        # 1. Detect document type
        doc_type = self.detect_document_type(pdf_path)
        
        # 2. Route to appropriate processor
        if doc_type == 'pci_dss':
            processor = PCIDSSProcessor()
        elif doc_type == 'iso27001':
            processor = ISO27001Processor()
        
        # 3. Extract content
        controls = processor.extract_controls(pdf_path)
        
        # 4. Generate dual outputs
        kb_chunks = BedrockFormatter().format_for_kb(controls)
        db_data = DatabaseFormatter().format_for_postgres(controls)
        
        # 5. Store in destinations
        self.store_in_bedrock_kb(kb_chunks)
        self.store_in_database(db_data)
        
        return ProcessingResult(controls=len(controls))
    
    def process_implementation_guide(self, pdf_path: str) -> ProcessingResult:
        """Process implementation guides like AWS best practices."""
        # 1. Use implementation guide processor
        processor = AWSPracticesProcessor()
        practices = processor.extract_practices(pdf_path)
        
        # 2. Map to compliance requirements
        mapper = ComplianceMapper()
        mapped_practices = mapper.map_to_pci_controls(practices)
        
        # 3. Generate outputs
        kb_chunks = BedrockFormatter().format_practices_for_kb(mapped_practices)
        db_data = DatabaseFormatter().format_practices_for_postgres(mapped_practices)
        
        # 4. Store
        self.store_in_bedrock_kb(kb_chunks)
        self.store_in_database(db_data)
        
        return ProcessingResult(practices=len(practices))
```

### **B/ Evidence Collection Workflow**
```python
# orchestration/workflows/evidence_workflow.py
class EvidenceCollectionWorkflow:
    
    def collect_multi_account_evidence(self, accounts: List[str]) -> CollectionResult:
        """Collect evidence from multiple AWS accounts."""
        # 1. Collect from all accounts
        collector = EvidenceCollector()
        evidence_set = collector.collect_aws_configs(accounts)
        
        # 2. Process and normalize
        processor = EvidenceProcessor()
        processed = processor.normalize_and_validate(evidence_set)
        
        # 3. Map to compliance requirements
        mapper = ComplianceMapper()
        compliance_evidence = mapper.map_to_requirements(processed)
        
        # 4. Generate outputs
        # S3 first for archival
        s3_package = StorageFormatter().package_evidence(compliance_evidence)
        S3Storage().upload_evidence_package(s3_package)
        
        # Then database for querying
        db_data = DatabaseFormatter().format_evidence_for_postgres(compliance_evidence)
        DatabaseIngestion().store_evidence(db_data)
        
        return CollectionResult(
            accounts=len(accounts),
            evidence_items=len(compliance_evidence),
            s3_location=s3_package.location,
            db_records=len(db_data)
        )
```

## ğŸ¯ **Immediate Implementation Plan**

### **Week 1-2: AWS Best Practices Processor**
```python
# ğŸš§ Priority 1: Build AWS practices processor
data_pipeline/processors/documents/implementation_guides/aws_practices_processor.py

class AWSPracticesProcessor:
    def extract_practices(self, pdf_path: str) -> List[ImplementationPractice]:
        """Extract AWS implementation practices from PDF."""
        # Different from PCI DSS - focuses on code examples, configurations
        pass
    
    def map_to_pci_controls(self, practices: List[ImplementationPractice]) -> List[MappedPractice]:
        """Map AWS practices to PCI DSS controls."""
        # Creates relationships between implementation and requirements
        pass
```

### **Week 3-4: Evidence Collector Enhancement**
```python
# ğŸš§ Priority 2: Build evidence collector
services/evidence_collector/collectors/aws_config_collector.py

class AWSConfigCollector:
    def __init__(self, assume_role_arn: str = None):
        self.session = boto3.Session()
        if assume_role_arn:
            self.session = self.assume_cross_account_role(assume_role_arn)
    
    def collect_ec2_evidence(self) -> EC2Evidence:
        """Collect EC2 security configurations."""
        ec2 = self.session.client('ec2')
        # Collect security groups, NACLs, instances, etc.
        pass
    
    def collect_vpc_evidence(self) -> VPCEvidence:
        """Collect VPC network configurations."""
        # Collect VPC settings, subnets, route tables, etc.
        pass
```

### **Week 5-6: Dual Output Integration**
```python
# ğŸš§ Priority 3: Enhance formatters for dual output
data_pipeline/formatters/bedrock/semantic_chunker.py
data_pipeline/formatters/database/postgres_formatter.py

# Both should work from same processed data
class DualOutputFormatter:
    def format_for_both_destinations(self, data: ProcessedData) -> DualOutput:
        bedrock_chunks = self.format_for_bedrock(data)
        postgres_data = self.format_for_postgres(data)
        return DualOutput(bedrock_chunks, postgres_data)
```

## ğŸŠ **Key Architectural Benefits**

### 1. **Document-Type-Specific Processing**
- âœ… **Standards Documents**: PCI DSS (306 controls) - working
- ğŸš§ **Implementation Guides**: AWS best practices - different parsing needed
- ğŸ”® **Future Extensible**: ISO27001, NIST CSF, Azure guides

### 2. **Local-First with Cloud Migration Path**
- âœ… **Current**: Process local PDFs
- ğŸ”„ **Migration Ready**: Easy to add S3 source handlers later
- ğŸš€ **Event-Driven**: Can add S3 triggers when ready

### 3. **Dual Output Optimization**
- ğŸ§  **Bedrock KB**: Semantic chunks optimized for RAG
- ğŸ—„ï¸ **PostgreSQL**: Structured data for compliance reporting
- ğŸ“¦ **S3 Archive**: Evidence storage and backup

### 4. **Evidence Collection Integration**
- âš™ï¸ **Config Collection**: Multi-account AWS config gathering
- ğŸ”„ **Processing**: Normalize and map to compliance requirements
- ğŸ’¾ **Dual Storage**: PostgreSQL for querying + S3 for archival

This architecture **matches your exact workflow** and scales naturally as you move to cloud infrastructure! ğŸš€ 